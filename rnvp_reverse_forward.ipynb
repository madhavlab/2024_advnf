{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c21ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import data_loader\n",
    "\n",
    "from utils import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model , regularizers\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "tfkd= tf.keras.datasets\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "os.makedirs('./ring_samples_sigmoid/100_set', exist_ok=True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[2],'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 32\n",
    "temp_val = np.linspace(0.05,2.05,num_classes)\n",
    "\n",
    "xy_data = np.float32(data_loader.load_data_mh_generated('./data/8x8_gibbslattices.pkl'))/(2*np.pi)\n",
    "\n",
    "trainset  = []\n",
    "testset   = []\n",
    "\n",
    "# Earlier experiment were on 5120 samples and 2048 samples set for validation set.\n",
    "# Second set experiments were conducted on 1024 samples and 1024 samples set for validation set.\n",
    "\n",
    "for i in range(num_classes):\n",
    "    trainset.append(xy_data[10000*i:10000*i+5000:50])\n",
    "    testset.append(xy_data[10000*i+9000:10000*(i+1):10])\n",
    "\n",
    "trainset = np.reshape(np.array(trainset),(-1,8,8,1))\n",
    "testset  = np.reshape(np.array(testset),(-1,8,8,1))\n",
    "\n",
    "train_temp = np.repeat(temp_val,100)\n",
    "test_temp  = np.repeat(temp_val,100)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_T    = tf.cast(np.repeat(train_temp,8*8).reshape(-1,8,8,1),dtype = tf.float32)\n",
    "test_T     = tf.cast(np.repeat(test_temp,8*8).reshape(-1,8,8,1),dtype = tf.float32)\n",
    "train_temp = tf.cast(train_temp.reshape(-1,),dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((trainset,train_T,train_temp))\n",
    "training_dataset = training_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "    \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testset,test_T))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128825b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkerboard(height, width, reverse=False, dtype=tf.float32):\n",
    "    checkerboard = [[((i % 2) + j) % 2 for j in range(width)] for i in range(height)] \n",
    "    checkerboard = tf.convert_to_tensor(checkerboard, dtype = dtype)\n",
    "    if reverse:\n",
    "        checkerboard = 1 - checkerboard\n",
    "    \n",
    "    checkerboard = tf.reshape(checkerboard, (1,height,width,1))\n",
    "        \n",
    "    return tf.cast(checkerboard, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 64\n",
    "\n",
    "def Coupling(input_shape):\n",
    "    input1 = keras.layers.Input(shape=input_shape)\n",
    "    input2 = keras.layers.Input(shape=input_shape)\n",
    "    input  = tf.concat([input1,input2],axis=-1)\n",
    "    \n",
    "    layer1 = keras.layers.Conv2D(filters,3, activation=\"relu\",padding = 'valid',name = 'layer1')(periodic_padding(input,1))\n",
    "    layer2 = keras.layers.Conv2D(filters,3, activation=\"relu\",padding = 'valid',name = 'layer2')(periodic_padding(layer1,1))\n",
    "    t_layer= keras.layers.Conv2D(1,3,padding = 'valid',name = 't_layer')(periodic_padding(layer2))\n",
    "    s_layer= keras.layers.Conv2D(1,3,activation = 'tanh',padding = 'valid', name = 's_layer')(periodic_padding(layer2))\n",
    "\n",
    "    return keras.Model(inputs=[input1,input2], outputs=[s_layer, t_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNormal:\n",
    "    def __init__(self, loc, var):\n",
    "        self.dist = tfd.Normal(tf.reshape(loc,(-1,)), tf.reshape(var,(-1,)))\n",
    "        self.shape = loc.shape\n",
    "    def log_prob(self, x):\n",
    "        logp = self.dist.log_prob(tf.reshape(x,(x.shape[0], -1)))\n",
    "        return tf.reduce_sum(logp, axis=1)\n",
    "    def sample_n(self, batch_size , seed = None):\n",
    "        x = self.dist.sample((batch_size,),seed = seed)\n",
    "        return tf.reshape(x,(batch_size, *self.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_action(lattice):\n",
    "    xp = tf.roll(lattice,shift=1, axis=2)\n",
    "    xn = tf.roll(lattice,shift=-1, axis=2)\n",
    "    yp = tf.roll(lattice,shift=[0,1,0], axis=[0,1,2])\n",
    "    yn = tf.roll(lattice,shift=[0,-1,0], axis=[0,1,2])\n",
    "    H_matrix = -1*(tf.math.cos(2*np.pi*(xp-lattice)) + tf.math.cos(2*np.pi*(xn-lattice))+ tf.math.cos(2*np.pi*(yp-lattice)) + tf.math.cos(2*np.pi*(yn-lattice)))\n",
    "    energy  = tf.reduce_sum((H_matrix),axis =[1,2])/2\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(keras.Model):\n",
    "    def __init__(self, num_coupling_layers,input_shape , data_constraint):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "        self.distribution = SimpleNormal(tf.zeros((8,8)), tf.ones((8,8)))\n",
    "        self.masks = [checkerboard(input_shape[0],input_shape[1], reverse=False),checkerboard(input_shape[0],input_shape[1], reverse=True)]*(num_coupling_layers // 2)\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.layers_list = [Coupling(input_shape) for i in range(num_coupling_layers)]\n",
    "        self.data_constraint = data_constraint\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x, forward=True):\n",
    "        if forward:\n",
    "            (x1,x2) = x \n",
    "            alpha = tf.constant(self.data_constraint)\n",
    "            logq = self.distribution.log_prob(tf.reshape(x1,(-1,8,8)))\n",
    "            x,ldj1 = self.forward(x)\n",
    "            (x1,x2)= x\n",
    "            ldj2 = tf.math.softplus(x1) + tf.math.softplus(-x1) + tf.math.log(tf.constant(1.-2*self.data_constraint)) \n",
    "            ldj2 = tf.reduce_sum(ldj2,[1,2,3])\n",
    "            logq = logq - ldj1 + ldj2\n",
    "            x1   = (tf.math.sigmoid(x1) - alpha)/(1-2*alpha)\n",
    "            x = (x1,x2)\n",
    "            return x, logq\n",
    "        else:\n",
    "            (x1,x2) = x\n",
    "            x1 = self.data_constraint + (1-2*self.data_constraint)*x1\n",
    "            x1 = tf.math.log(x1/(1.-x1)) \n",
    "            # Save log-determinant of Jacobian of initial transform\n",
    "            ldj1 = tf.math.softplus(x1) + tf.math.softplus(-x1) + tf.math.log(tf.constant(1.-2*self.data_constraint))\n",
    "            ldj1 = tf.reduce_sum(ldj1,[1,2,3])\n",
    "            x = (x1,x2)\n",
    "            x,ldj2 = self.reverse(x)\n",
    "            (x1,x2) = x\n",
    "            logq = self.distribution.log_prob(tf.reshape(x1,(-1,8,8)))\n",
    "            logq = logq + ldj2 + ldj1\n",
    "            return x , logq   \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ldj = 0\n",
    "        for i in range(self.num_coupling_layers):\n",
    "            (x1,x2) = x\n",
    "            \n",
    "            x_frozen = x1 * self.masks[i]\n",
    "            reversed_mask = 1 - self.masks[i]\n",
    "            x_active = x1 * reversed_mask\n",
    "            s, t = self.layers_list[i]([x_frozen,x2])\n",
    "            s *= reversed_mask\n",
    "            t *= reversed_mask\n",
    "            \n",
    "            fx1 = t + x_active *tf.exp(s) + x_frozen\n",
    "            fx2 = x2\n",
    "            ldj += tf.reduce_sum(s, [1,2,3])\n",
    "            x = (fx1,fx2)\n",
    "        return x, ldj\n",
    "              \n",
    "    def reverse(self, fx):\n",
    "        ldj = 0\n",
    "        for i in reversed(range(self.num_coupling_layers)):\n",
    "            (fx1,fx2)= fx\n",
    "            fx_frozen = fx1*self.masks[i]\n",
    "            reversed_mask = 1 - self.masks[i]\n",
    "            fx_active = fx1*reversed_mask\n",
    "            s, t = self.layers_list[i]([fx_frozen,fx2])\n",
    "            s *= reversed_mask\n",
    "            t *= reversed_mask\n",
    "            \n",
    "            x1 = (fx_active - t) *tf.exp(-s) + fx_frozen\n",
    "            x2 = fx2\n",
    "            ldj -= tf.reduce_sum(s, [1,2,3])\n",
    "            fx = (x1,x2)\n",
    "        return fx,ldj\n",
    "        \n",
    "     \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RealNVP(24,(8,8,1),1.e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [1250*1, 1250*2, 1250*3, 1250*4, 1250*5, 1250*6, 1250*7, 1250*8, 1250*9, 1250*10]\n",
    " \n",
    "gen_lr = 5.e-5\n",
    "decay  = 0.95\n",
    "\n",
    "values_gen = [gen_lr, gen_lr*decay, gen_lr*(decay)**2, gen_lr*(decay)**3, gen_lr*(decay)**4,\n",
    "              gen_lr*(decay)**5, gen_lr*(decay)**6, gen_lr*(decay)**7, gen_lr*(decay)**8, \n",
    "              gen_lr*(decay)**9, gen_lr*(decay)**10]\n",
    "\n",
    "learning_rate_fn_gen = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values_gen)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn_gen)\n",
    "\n",
    "train_FKL_loss= tf.keras.metrics.Mean('train_FKL_loss',dtype=tf.float32)\n",
    "train_RKL_loss= tf.keras.metrics.Mean('train_RKL_loss',dtype=tf.float32)\n",
    "train_total_loss= tf.keras.metrics.Mean('train_total_loss',dtype=tf.float32)\n",
    "\n",
    "nll_loss  = tf.keras.metrics.Mean('nll_loss',dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c433ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = './ring_samples_sigmoid/100_set/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-10-2-6\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,generator = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x,y,T,model):\n",
    "    lambda1 = 0.5\n",
    "    lambda2 = 1.0\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        z  = tf.reshape(model.distribution.sample_n(y.shape[0]),(-1,8,8,1))\n",
    "        fx , logq = model((z,y),forward = True)\n",
    "        (fx1,fx2) = fx\n",
    "        logp = -xy_action(tf.reshape(fx1,(-1,8,8)))/T\n",
    "        reverse_loss = tf.reduce_mean(logq -logp) # Reverse Kl loss\n",
    "        x  , logq = model((x,y),forward = False)\n",
    "        forward_loss = -tf.reduce_mean(logq)  # Forward Kl loss\n",
    "        \n",
    "        total_loss = lambda1 * reverse_loss + lambda2 * forward_loss\n",
    "        \n",
    "    gradients = gen_tape.gradient(total_loss,model.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    train_FKL_loss(forward_loss)\n",
    "    train_RKL_loss(reverse_loss)\n",
    "    train_total_loss(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x,y,model,forward = False):\n",
    "    x , logq = model((x,y),forward = False)\n",
    "    nll = -tf.reduce_mean(logq)  # Forward Kl loss\n",
    "    nll_loss(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for epoch in range(3):\n",
    "    \n",
    "    for step,(x,y,T) in enumerate(training_dataset):\n",
    "        train_step(x,y,T,model)\n",
    "    \n",
    "    #template = 'Epoch {:2d}, Gen_total_loss: {:.6f},Gen_Loss: {:.6f},kl_loss: {:.6f}, Disc_Loss: {:.6f}'\n",
    "    #print (template.format(epoch+1,train_G_net_loss.result(),train_G_loss.result(),train_KL_loss.result(),train_D_loss.result()))\n",
    "\n",
    "    for step,(x,y) in enumerate(test_dataset):\n",
    "        test_step(x,y,model,forward = False)\n",
    "        \n",
    "    template = 'Epoch {:3d},total_loss: {:.6f},forward_loss: {:.6f},reverse_loss: {:.6f},test_loss: {:.6f}'\n",
    "    print (template.format(epoch+1,train_total_loss.result(),train_FKL_loss.result(),train_RKL_loss.result(),nll_loss.result()))\n",
    "\n",
    "    # Reset metrics every epoch\n",
    "    \n",
    "    train_FKL_loss.reset_states()\n",
    "    train_RKL_loss.reset_states()\n",
    "    train_total_loss.reset_states()\n",
    "    nll_loss.reset_states()\n",
    "       \n",
    "        \n",
    "stop = time.time()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "print('Time: ', stop - start)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2da0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196dd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from Model\n",
    "samples = []\n",
    "for i in range(num_classes):\n",
    "    cond = temp_val[i]*tf.ones(shape = [1000,8,8,1],dtype=tf.float32)\n",
    "    z = tf.reshape(model.distribution.sample_n(1000,seed = 1000*i),(-1,8,8,1))\n",
    "    x,logq = model((z,cond),forward = True)\n",
    "    samples.append(x[0])\n",
    "samples = np.array(samples).reshape((-1,8,8,1))\n",
    "print(samples.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect only 1000 samples for comparison \n",
    "#size of lattices = 8x8\n",
    "#MCMC Generated lattices has 10000 samples against each lattice.\n",
    "\n",
    "lat = []\n",
    "for i in range(32):\n",
    "    lat.append(xy_data[10000*i+4500:10000*i+5500])\n",
    "    \n",
    "mcmc_samples = np.array(lat).reshape((-1,8,8,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f443387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('./cgan/igan.pkl', 'rb')\n",
    "# cgan_data = pickle.load(f,encoding=\"latin1\")\n",
    "# cgan_data = np.float32(np.array(cgan_data).reshape((-1,8,8,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cgan_data = np.reshape(np.arctan2(cgan_data[:,:,:,1],cgan_data[:,:,:,0])/(2*np.pi),(-1,8,8,1))\n",
    "# comparison_plot(mcmc_samples,cgan_data,1000,0.05,2.05,32,J=1,K=1,name='./ring_samples_sigmoid/igan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot(mcmc_samples,samples,1000,0.05,2.05,32,J=1,K=1,name='./ring_samples_sigmoid/100_set/model_comparison_plot_ckpt_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(mcmc_samples,samples,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_sample_generator(model,temp, action, batch_size, N_samples, seed = 500):\n",
    "    x1, logq, logp = None, None, None\n",
    "       \n",
    "    for i in range(N_samples):\n",
    "        batch_i = i % batch_size\n",
    "        if batch_i == 0:\n",
    "        # we're out of samples to propose, generate a new batch\n",
    "            #seed = (np.rint(seed + batch_i + 1000*temp)).astype(int)\n",
    "            #print(seed.dtype)\n",
    "            z = tf.reshape(model.distribution.sample_n(batch_size,seed = seed + batch_i),(-1,8,8,1))\n",
    "            cond = temp*tf.ones((batch_size,8,8,1))\n",
    "            x, logq = model((z,cond),forward = True)\n",
    "            (x1,x2) = x\n",
    "            logp = -action(x1)/temp\n",
    "        yield x1[batch_i], logq[batch_i], logp[batch_i]\n",
    "        \n",
    "def make_mcmc_ensemble(model, action, batch_size, N_samples,seed):\n",
    "    rs = np.random.RandomState(seed=1000)\n",
    "    history_for_all_temp = {'x' : [],'logq' : [],'logp' : [],'accepted' : [],'logw' : []}\n",
    "    # build Markov chain\n",
    "    temp_val = np.linspace(0.05,2.05,num_classes)\n",
    "    for i,temp in enumerate(temp_val):\n",
    "        history = {'x' : [],'logq' : [],'logp' : [],'accepted' : [],'logw' : []}\n",
    "        sample_gen = serial_sample_generator(model,temp, action, batch_size, N_samples,seed = seed)\n",
    "        \n",
    "        for new_x, new_logq, new_logp in sample_gen:\n",
    "            if len(history['logp']) == 0:\n",
    "                # always accept first proposal, Markov chain must start somewhere\n",
    "                accepted = True\n",
    "            else:\n",
    "                # Metropolis acceptance condition\n",
    "                last_logp = history['logp'][-1]\n",
    "                last_logq = history['logq'][-1]\n",
    "                p_accept = tf.math.exp((new_logp - new_logq) - (last_logp - last_logq))\n",
    "                p_accept = min(1, p_accept)\n",
    "                draw = rs.rand() # ~ [0,1]\n",
    "                if draw < p_accept:\n",
    "                    accepted = True\n",
    "                    \n",
    "                else:\n",
    "                    accepted = False\n",
    "                    new_x = history['x'][-1]\n",
    "                    new_logp = last_logp\n",
    "                    new_logq = last_logq\n",
    "            # Update Markov chain\n",
    "            history['logp'].append(new_logp)\n",
    "            history['logq'].append(new_logq)\n",
    "            history['x'].append(new_x)\n",
    "            history['accepted'].append(accepted)\n",
    "            history['logw'].append(new_logp - new_logq)\n",
    "            history_for_all_temp['logp'].append(new_logp)\n",
    "            history_for_all_temp['logq'].append(new_logq)\n",
    "            history_for_all_temp['x'].append(new_x)\n",
    "            history_for_all_temp['accepted'].append(accepted)\n",
    "            history_for_all_temp['logw'].append(new_logp - new_logq)\n",
    "    return history_for_all_temp\n",
    "\n",
    "ensemble_size = 1000\n",
    "xy_ens = make_mcmc_ensemble(model, xy_action, 256, ensemble_size , seed = 1000)\n",
    "print(\"Accept rate:\", 100*np.mean(xy_ens['accepted']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_samples  = np.array(xy_ens['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f48358",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(mcmc_samples,mh_samples,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot(mcmc_samples,mh_samples,1000,0.05,2.05,32,J=1,K=1,name='./ring_samples_sigmoid/100_set/mh_comparison_plot_ckpt_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81548d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*tf.math.reduce_logsumexp(logw, axis=0) - tf.math.reduce_logsumexp(2*logw, axis=0)\n",
    "    ess_per_cfg = tf.math.exp(log_ess) / len(logw)\n",
    "    return ess_per_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_mh = []\n",
    "AR     = []\n",
    "\n",
    "for j in range(5):\n",
    "    xy_ens = make_mcmc_ensemble(model, xy_action , 256,1000,seed = 1000*j) \n",
    "#     logp_acc = np.array(mog_ens['logp']).reshape((-1,))\n",
    "#     logq_acc = np.array(mog_ens['logq']).reshape((-1,))\n",
    "#     ess_mh.append(compute_ess(logp_acc,logq_acc).numpy())\n",
    "    logw = np.array(xy_ens['logw']).reshape((-1,))\n",
    "    log_ess = 2*tf.math.reduce_logsumexp(logw, axis=0) - tf.math.reduce_logsumexp(2*logw, axis=0)\n",
    "    ess_per_cfg = tf.math.exp(log_ess) / len(logw)\n",
    "    ess_mh.append(ess_per_cfg.numpy())\n",
    "    AR.append(100*np.mean(xy_ens['accepted']))\n",
    "\n",
    "print('ESS-MH:',ess_mh)\n",
    "print('AR :',AR)\n",
    "print('ESS-MH Mean : ',np.mean(np.array(ess_mh)))\n",
    "print('AR Mean :',np.mean(np.array(AR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24880687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = []\n",
    "for i in range(5):\n",
    "    samples = []\n",
    "    logp_array = []\n",
    "    logq_array = []\n",
    "    for j in range(num_classes):\n",
    "        cond = temp_val[i]*tf.ones(shape = [1000,8,8,1],dtype=tf.float32)\n",
    "        z = tf.reshape(model.distribution.sample_n(1000,seed = 1000*i+j),(-1,8,8,1))\n",
    "        x,logq = model((z,cond),forward = True)\n",
    "        samples.append(x[0])\n",
    "        logp = -xy_action(x[0])/temp_val[i]\n",
    "        logp_array.append(logp)\n",
    "        logq_array.append(logq)\n",
    "        \n",
    "    samples = np.array(samples).reshape((-1,8,8,1))\n",
    "    logp = np.array(logp_array).reshape((-1,))\n",
    "    logq = np.array(logq_array).reshape((-1,))\n",
    "    ess.append(compute_ess(logp, logq).numpy()) \n",
    "\n",
    "print('ESS:',ess)\n",
    "print('ESS Mean : ',np.mean(np.array(ess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(num_classes):\n",
    "    test_data.append(xy_data[10000*i+9000:10000*(i+1)])\n",
    "\n",
    "test_data  = np.reshape(np.array(test_data),(-1,8,8,1))\n",
    "cond_temp  = np.repeat(temp_val,1000)\n",
    "\n",
    "\n",
    "batch_size   = 1000\n",
    "\n",
    "test_cond    = tf.cast(np.repeat(cond_temp,8*8).reshape(-1,8,8,1),dtype = tf.float32)\n",
    "test_set     = tf.data.Dataset.from_tensor_slices((test_data,test_cond))\n",
    "test_set     = test_set.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = []\n",
    "for step,(x,y) in enumerate(test_set):\n",
    "    x , logq = model((x,y),forward = False)\n",
    "    nll = -tf.reduce_mean(logq)\n",
    "    NLL.append(nll.numpy())\n",
    "NLL = np.array(NLL)\n",
    "print('NLL Mean :',np.mean(NLL))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9ba23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b51b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659d63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
